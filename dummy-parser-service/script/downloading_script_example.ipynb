{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "    \n",
    "def parse_rbc_articles(url):\n",
    "    headers = {\n",
    "        \"accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "        \"accept-language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"sec-ch-ua\": \"\\\"Not A(Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"121\\\", \\\"Chromium\\\";v=\\\"121\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-origin\",\n",
    "        \"x-requested-with\": \"XMLHttpRequest\"\n",
    "    }\n",
    "\n",
    "    status_code = -1\n",
    "    response = None\n",
    "    while status_code == -1:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                return json.dumps({\"error\": \"Failed to fetch the data from the URL provided.\"})\n",
    "            status_code = response.status_code\n",
    "        except Exception as e:\n",
    "            time.sleep(30)\n",
    "\n",
    "    data = response.json()\n",
    "    articles_list = []\n",
    "    for item in data['items']:\n",
    "        article = {\n",
    "            \"header\": item.get('title', ''),\n",
    "            \"published\": datetime.fromtimestamp(item.get('publish_date_t', 0)).strftime(\n",
    "                '%Y-%m-%d %H:%M:%S') if item.get('publish_date_t') else '',\n",
    "            \"summary\": item.get('body', ''),\n",
    "            \"ticker\": ''\n",
    "        }\n",
    "        articles_list.append(article)\n",
    "\n",
    "    return articles_list\n",
    "\n",
    "def parse_tradingview_headlines(url):\n",
    "    headers = {\n",
    "        \"accept\": \"*/*\",\n",
    "        \"accept-language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"sec-ch-ua\": \"\\\"Not A(Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"121\\\", \\\"Chromium\\\";v=\\\"121\\\"\",\n",
    "        \"sec-ch-ua-mobile\": \"?0\",\n",
    "        \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "        \"sec-fetch-dest\": \"empty\",\n",
    "        \"sec-fetch-mode\": \"cors\",\n",
    "        \"sec-fetch-site\": \"same-site\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return json.dumps({\"error\": \"Failed to fetch the data from the URL provided.\"})\n",
    "\n",
    "    data = response.json()\n",
    "    headlines_list = []\n",
    "    for item in data['items']:\n",
    "        publish_time = datetime.fromtimestamp(item['published']).strftime('%Y-%m-%d %H:%M:%S') if item.get(\n",
    "            'published') else ''\n",
    "        ticker = ', '.join([symbol['symbol'] for symbol in item.get('relatedSymbols', []) if 'symbol' in symbol])\n",
    "\n",
    "        headline = {\n",
    "            \"header\": item.get('title', ''),\n",
    "            \"published\": publish_time,\n",
    "            \"summary\": '',\n",
    "            \"ticker\": ticker\n",
    "        }\n",
    "        headlines_list.append(headline)\n",
    "\n",
    "    return headlines_list\n",
    "\n",
    "def parse_url(url):\n",
    "    if \"rbc\" in url:\n",
    "        return parse_rbc_articles(url)\n",
    "    elif \"tradingview\" in url:\n",
    "        return parse_tradingview_headlines(url)\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700146c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"tinkof api token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinkoff.invest import RequestError, Quotation, Client, CandleInterval\n",
    "from tinkoff.invest.grpc.orders_pb2 import ORDER_DIRECTION_BUY, ORDER_DIRECTION_SELL, ORDER_TYPE_BESTPRICE\n",
    "from tinkoff.invest.utils import now\n",
    "\n",
    "\n",
    "def convert_quotation_to_float(units: int, nano: int):\n",
    "    return units + (nano / (10 ** 9))\n",
    "\n",
    "\n",
    "def float_to_quotation(f: float) -> Quotation:\n",
    "    float_str = str(f)\n",
    "    units = int(float_str[:float_str.find('.')])\n",
    "\n",
    "    if len(float_str[float_str.find('.')+1:]) > 9:\n",
    "        end = 9 - len(float_str[float_str.find('.')+1:])\n",
    "        nano = int(float_str[float_str.find('.')+1:end])\n",
    "    elif len(str(float_str[float_str.find('.')+1:])) < 9:\n",
    "        nano = int(int(int(float_str[float_str.find('.')+1:]) * 1_000_000_000) // pow(10, 9 - (9 - len(float_str[float_str.find('.')+1:]))))\n",
    "    else:\n",
    "        nano = int(float_str[float_str.find('.')+1:])\n",
    "\n",
    "    print(\"Str: \" + float_str + \" units: \" + str(units) + \" nano: \" + str(nano))\n",
    "    return Quotation(units=units, nano=nano)\n",
    "\n",
    "\n",
    "def round_to_near_min_increment(min_inc, price):\n",
    "    return price - (price % min_inc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_ticker = {}\n",
    "ticker_to_uid = {}\n",
    "uid_to_ticker = {}\n",
    "lots = {}\n",
    "min_price_increment = {}\n",
    "\n",
    "with Client(TOKEN) as client:\n",
    "    for method in ['shares']:\n",
    "        for item in getattr(client.instruments, method)().instruments:\n",
    "            if item.for_qual_investor_flag is False and item.api_trade_available_flag is True and item.buy_available_flag is True and item.currency == 'rub' and item.country_of_risk == 'RU':\n",
    "                name_to_ticker[item.name] = item.ticker\n",
    "                ticker_to_uid[item.ticker] = item.uid\n",
    "                uid_to_ticker[item.uid] = item.ticker\n",
    "                lots[item.ticker] = item.lot\n",
    "                min_price_increment[item.ticker] = convert_quotation_to_float(\n",
    "                    units=item.min_price_increment.units, nano=item.min_price_increment.nano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 новостей от trading view\n",
    "tv_articles = {}\n",
    "for ticker in ticker_to_uid.keys():\n",
    "    print(ticker)\n",
    "    news = parse_url(\"https://news-headlines.tradingview.com/v2/view/headlines/symbol?client=web&lang=ru&streaming=true&symbol=MOEX%3A\" + ticker)\n",
    "    tv_articles[ticker] = []\n",
    "    for item in news:\n",
    "        try:\n",
    "            published_dt = datetime.strptime(item['published'], '%Y-%m-%d %H:%M:%S')\n",
    "            tv_articles[ticker].append((item['header'], published_dt))\n",
    "            print(f\"New article found: {item['header']} published at {item['published']}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa9cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Проверяем существует ли папка 'data', и если нет, создаем ее\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "for ticker, articles in tv_articles.items():\n",
    "    # Изменяем путь к файлу для сохранения в папке 'data'\n",
    "    filename = os.path.join('data', f\"tv_{ticker}.csv\")\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['header', 'published'])  # Запись заголовков столбцов\n",
    "        for header, published_dt in articles:\n",
    "            # Преобразуем datetime объект обратно в строку для записи\n",
    "            published_str = published_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            writer.writerow([header, published_str])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5901ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверям недоступные имена. Если недоступно, то печатаем и удаляем из списка.\n",
    "\n",
    "direct_available_names = {}\n",
    "headers = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"sec-ch-ua\": \"\\\"Not A(Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"121\\\", \\\"Chromium\\\";v=\\\"121\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-site\"\n",
    "}\n",
    "\n",
    "for company_name, ticker in name_to_ticker.items():\n",
    "    url = \"https://www.rbc.ru/tags/?tag=\" + company_name\n",
    "    #url = \"https://www.rbc.ru/search/ajax/?tag=\" + company_name + \"&project=rbcnews&page=1\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(company_name + \" not avaliable  ticker: \" + ticker)\n",
    "    else:\n",
    "        direct_available_names[company_name] = ticker\n",
    "        print(company_name + \" available\")\n",
    "\n",
    "print(direct_available_names)\n",
    "# руками вставляем нужные акции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_available_names[\"En+ Group\"] = \"ENPG\"\n",
    "direct_available_names[\"ВКонтакте\"] = \"VKCO\"\n",
    "direct_available_names[\"«Таттелеком»\"] = \"TTLK\"\n",
    "direct_available_names[\"ММК\"] = \"MAGN\"\n",
    "direct_available_names[\"ВТБ\"] = \"VTBR\"\n",
    "direct_available_names[\"ЦИАН\"] = \"CIAN\"\n",
    "direct_available_names[\"Сбербанк\"] = \"SBER\"\n",
    "direct_available_names[\"X5\"] = \"FIVE\"\n",
    "direct_available_names[\"Черкизово\"] = \"GCHE\"\n",
    "direct_available_names[\"Тинькофф Банк\"] = \"TCSG\"\n",
    "direct_available_names[\"Ozon\"] = \"OZON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4473c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbc_news_artictles = {}\n",
    "\n",
    "for company_name, ticker in direct_available_names.items():\n",
    "    rbc_news_artictles[ticker] = []\n",
    "    page = 0\n",
    "    news = parse_url(\"https://www.rbc.ru/search/ajax/?tag=\" + company_name + \"&project=rbcnews&page=\" + str(page))\n",
    "    while len(news) != 0:\n",
    "        for item in news:\n",
    "            try:\n",
    "                published_dt = datetime.strptime(item['published'], '%Y-%m-%d %H:%M:%S')\n",
    "                rbc_news_artictles[ticker].append((item['header'], published_dt))\n",
    "                print(f\"New article found: {item['header']} published at {item['published']}\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "        page += 1\n",
    "        news = parse_url(\"https://www.rbc.ru/search/ajax/?tag=\" + company_name + \"&project=rbcnews&page=\" + str(page))\n",
    "        time.sleep(1)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Проверяем существует ли папка 'data', и если нет, создаем ее\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "for ticker, articles in rbc_news_artictles.items():\n",
    "    # Изменяем путь к файлу для сохранения в папке 'data'\n",
    "    filename = os.path.join('data', f\"rbc_news_{ticker}.csv\")\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['header', 'published'])  # Запись заголовков столбцов\n",
    "        for header, published_dt in articles:\n",
    "            # Преобразуем datetime объект обратно в строку для записи\n",
    "            published_str = published_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            writer.writerow([header, published_str])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d70e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def generate_declensions_with_full_name(name, stop_words):\n",
    "    declensions = set([name.lower()])  # Добавляем полное название в нижнем регистре\n",
    "    for word in name.split():\n",
    "        word = word.lower()  # Приведение к нижнему регистру\n",
    "        if word not in stop_words and len(word) > 3:  # Игнорирование стоп-слов\n",
    "            parsed_word = morph.parse(word)[0]\n",
    "            declensions.add(parsed_word.normal_form)\n",
    "            # Добавление всех форм слова в набор склонений\n",
    "            for form in parsed_word.lexeme:\n",
    "                declensions.add(form.word)\n",
    "    return declensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d059ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\"акции\", \"привилегированные\", \"обыкновенные\", \"и\", \"-\", \"банк\", \"гдр\", \"group\", \"группа\", \"мир\", \"мира\", \"биржа\", 'компания',\n",
    "             \"красный\", \"октябрь\", \"сеть\", \"сети\", \"рынка\", \"сетями\", \"московская\", \"московской\", \"москва\", \"московский\", \"адр\", \"второй\",\n",
    "              \"вторая\", \"интерфакс\", \"система\", \"в\", \"московском\", \"регион\", \"нефть\"}\n",
    "\n",
    "# Генерация склонений для каждой компании и сохранение их в новый словарь\n",
    "company_declensions = {company: generate_declensions_with_full_name(company, stop_words) for company in direct_available_names.keys()}\n",
    "for company in name_to_ticker.keys():\n",
    "    additional_declensions = generate_declensions_with_full_name(company, stop_words)\n",
    "    # Объединение наборов склонений, если компания уже присутствует в словаре\n",
    "    if company in company_declensions:\n",
    "        company_declensions[company] = company_declensions[company].union(additional_declensions)\n",
    "    else:\n",
    "        company_declensions[company] = additional_declensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Добавление кавычек в список удаляемых символов\n",
    "    punctuation_extended = string.punctuation + '«»“”‘’'  # Учитывает разные типы кавычек\n",
    "    return text.translate(str.maketrans('', '', punctuation_extended)).lower()\n",
    "\n",
    "def check_companies_in_text(text, stop_words):\n",
    "    text = preprocess_text(\" \" + text.lower() + \" \")  # Предобработка и приведение к нижнему регистру\n",
    "    found_companies = {}\n",
    "\n",
    "    for company, declensions in company_declensions.items():\n",
    "        # Проверка на полное вхождение названия компании в текст\n",
    "        if \" \" + company.lower() + \" \" in text:\n",
    "            # Если полное название компании найдено, добавляем его в результат\n",
    "            if company in direct_available_names:\n",
    "                found_companies[company] = direct_available_names[company]\n",
    "            elif company in name_to_ticker:\n",
    "                found_companies[company] = name_to_ticker[company]\n",
    "        else:\n",
    "            # Проверка на вхождение склоненных форм названия в тексте\n",
    "            for declension in declensions:\n",
    "                if \" \" + declension.lower() + \" \" in text:  # Изменено на проверку вхождения во всём тексте\n",
    "                    if company in direct_available_names:\n",
    "                        found_companies[company] = direct_available_names[company]\n",
    "                    elif company in name_to_ticker:\n",
    "                        found_companies[company] = name_to_ticker[company]\n",
    "                    break  # Прерываем цикл после первого найденного совпадения\n",
    "\n",
    "    return found_companies\n",
    "\n",
    "\n",
    "text = \"Сегодня акции Fix Price показали рост, также как и бумаги Селигдар.\"\n",
    "found_companies = check_companies_in_text(text, stop_words)\n",
    "print(\"Найденные компании и тикеры:\", found_companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1481b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Заголовки запроса\n",
    "headers = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"sec-ch-ua\": \"\\\"Not A(Brand\\\";v=\\\"99\\\", \\\"Google Chrome\\\";v=\\\"121\\\", \\\"Chromium\\\";v=\\\"121\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-site\"\n",
    "}\n",
    "\n",
    "alenka_news_artictles = {}\n",
    "\n",
    "# Начальная дата\n",
    "date = datetime(2024, 3, 7)\n",
    "\n",
    "# Конечная дата\n",
    "end_date = datetime(2020, 1, 1)\n",
    "\n",
    "# Пока текущая дата больше или равна конечной дате\n",
    "while date >= end_date:\n",
    "    try:\n",
    "        # Формируем URL с текущей датой\n",
    "        formatted_date = date.strftime('%d.%m.%Y')\n",
    "        url = f\"https://alenka.capital/category/view_596/?date={formatted_date}\"\n",
    "\n",
    "        # Отправляем HTTP запрос\n",
    "        response = requests.get(url, headers=headers)  # Предполагается, что headers уже определены\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            news_items = soup.find_all('li', class_='feed__item')\n",
    "\n",
    "            for item in news_items:\n",
    "                time_published = item.find('time', class_='feed__date').text.strip()\n",
    "                news_title = item.find('h2', class_='feed__text').text.strip()\n",
    "\n",
    "                # Предполагается, что функция check_companies_in_text и словарь alenka_news_artictles уже определены\n",
    "                related_tickers = check_companies_in_text(news_title, stop_words)\n",
    "                published_dt = datetime.strptime(time_published, '%d.%m.%Y, %H:%M')\n",
    "                formatted_published_dt = published_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                for ticker in related_tickers.values():\n",
    "                    if ticker not in alenka_news_artictles:\n",
    "                        alenka_news_artictles[ticker] = []\n",
    "                    alenka_news_artictles[ticker].append((news_title, formatted_published_dt))\n",
    "                print(f\"{time_published} - {news_title}\")\n",
    "        else:\n",
    "            print(f\"Страница за {formatted_date} не доступна\")\n",
    "\n",
    "        # Уменьшаем дату на один день\n",
    "        date -= timedelta(days=1)\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Проверяем существует ли папка 'data', и если нет, создаем ее\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "for ticker, articles in alenka_news_artictles.items():\n",
    "    # Изменяем путь к файлу для сохранения в папке 'data'\n",
    "    filename = os.path.join('data', f\"alenka_news_{ticker}.csv\")\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['header', 'published'])  # Запись заголовков столбцов\n",
    "        for header, published_dt in articles:\n",
    "            writer.writerow([header, published_dt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a85a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'check_companies_in_text' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 51\u001B[0m\n\u001B[1;32m     49\u001B[0m formatted_published_dt \u001B[38;5;241m=\u001B[39m published_dt_obj\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 51\u001B[0m related_tickers \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_companies_in_text\u001B[49m(title, stop_words)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ticker \u001B[38;5;129;01min\u001B[39;00m related_tickers\u001B[38;5;241m.\u001B[39mvalues():\n",
      "\u001B[0;31mNameError\u001B[0m: name 'check_companies_in_text' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 71\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mprint\u001B[39m(e)\n\u001B[0;32m---> 71\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Задержка при возникновении исключения\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import time\n",
    "\n",
    "tass_news_articles = {}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"accept-language\": \"ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"122\\\", \\\"Not(A:Brand\\\";v=\\\"24\\\", \\\"Google Chrome\\\";v=\\\"122\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\"\n",
    "}\n",
    "\n",
    "last_updated = \"2024-03-10T00:00:00.028547\"\n",
    "end_date = datetime(2022, 1, 1)\n",
    "last_valid_datetime = None\n",
    "max_backward_jump = timedelta(days=2)  # Максимально допустимый скачок назад (например, 1 год)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        url = f\"https://tass.ru/tbp/api/v1/content?limit=20&lang=ru&rubrics=/ekonomika&sort=-es_updated_dt&last_es_updated_dt={last_updated}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            if not data['result']:\n",
    "                break  # Выходим из цикла, если новостей больше нет\n",
    "            \n",
    "            first_news_dt = datetime.fromisoformat(data['result'][0]['published_dt'].rstrip(\"Z\"))\n",
    "            if last_valid_datetime and first_news_dt > last_valid_datetime:\n",
    "                # Если время \"соскочило\" вперед, корректируем на час назад и повторяем запрос\n",
    "                last_updated = (last_valid_datetime - timedelta(hours=1)).isoformat() + \"Z\"\n",
    "                continue\n",
    "                \n",
    "            if last_valid_datetime and first_news_dt < last_valid_datetime - max_backward_jump:\n",
    "                # Корректируем last_updated на один час вперед, чтобы попытаться избежать большого скачка назад\n",
    "                last_updated = (last_valid_datetime - timedelta(hours=1)).isoformat() + \"Z\"\n",
    "                continue\n",
    "                \n",
    "            for item in data['result']:\n",
    "                title = item['title']\n",
    "                published_dt = item['published_dt']\n",
    "                published_dt_obj = datetime.fromisoformat(published_dt.rstrip(\"Z\"))\n",
    "                formatted_published_dt = published_dt_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                \n",
    "                related_tickers = check_companies_in_text(title, stop_words)\n",
    "\n",
    "                for ticker in related_tickers.values():\n",
    "                    if ticker not in tass_news_articles:\n",
    "                        tass_news_articles[ticker] = []\n",
    "                    tass_news_articles[ticker].append((title, formatted_published_dt))\n",
    "\n",
    "                \n",
    "                print(f\"Заголовок: {title}\\nДата публикации: {formatted_published_dt}\\n\")\n",
    "                \n",
    "            # Обновляем last_updated для следующего запроса\n",
    "            last_updated = data['result'][-1]['es_updated_dt']\n",
    "            last_valid_datetime = datetime.fromisoformat(last_updated.rstrip(\"Z\"))\n",
    "            \n",
    "            if last_valid_datetime <= end_date:\n",
    "                break  # Если достигли или перешли конечную дату, завершаем цикл\n",
    "        \n",
    "        #time.sleep(0.2)  # Задержка для соблюдения лимитов API\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(30)  # Задержка при возникновении исключения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Проверяем существует ли папка 'data', и если нет, создаем ее\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "for ticker, articles in tass_news_articles.items():\n",
    "    # Изменяем путь к файлу для сохранения в папке 'data'\n",
    "    filename = os.path.join('data', f\"tass_news_{ticker}.csv\")\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['header', 'published'])  # Запись заголовков столбцов\n",
    "        for header, published_dt in articles:\n",
    "            writer.writerow([header, published_dt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d749b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Функция для получения новостей с сайта Interfax за конкретный день\n",
    "def get_interfax_news_for_day(date):\n",
    "    date_str = date.strftime(\"%Y/%m/%d\")\n",
    "    url = f\"https://www.interfax.ru/business/news/{date_str}\"\n",
    "    response = requests.get(url)\n",
    "    news_list = []\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        news_items = soup.find_all('div', attrs={'data-id': True})\n",
    "\n",
    "        for item in news_items:\n",
    "            time_span = item.find('span')\n",
    "            # Форматируем время с добавлением 0 секунд для единобразия\n",
    "            news_time = datetime.strptime(f\"{date_str} {time_span.text}:00\", \"%Y/%m/%d %H:%M:%S\")\n",
    "            title_h3 = item.find('h3')\n",
    "            if title_h3:\n",
    "                news_title = title_h3.text\n",
    "                # Используем strftime для форматирования datetime объекта в строку с добавлением 0 секунд\n",
    "                formatted_news_time = news_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                news_list.append({'title': news_title, 'datetime': formatted_news_time})\n",
    "\n",
    "    return news_list\n",
    "\n",
    "# Функция для получения новостей в заданном диапазоне дат\n",
    "def get_interfax_news(start_date, end_date):\n",
    "    delta = timedelta(days=1)\n",
    "    current_date = start_date\n",
    "    all_news = []\n",
    "\n",
    "    while current_date <= end_date:\n",
    "        daily_news = get_interfax_news_for_day(current_date)\n",
    "        all_news.extend(daily_news)\n",
    "        current_date += delta\n",
    "\n",
    "    return all_news\n",
    "\n",
    "# Пример использования функций\n",
    "# Обратите внимание, что вызовы функций закомментированы, чтобы предотвратить их выполнение здесь.\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2024, 3, 9)\n",
    "news = get_interfax_news(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "interfacs_news_articles = {}\n",
    "for current_news in news:\n",
    "    print(\"Заголовок: \" + current_news[\"title\"] + \"\\nДата публикации: \" + current_news[\"datetime\"] + \"\\n\")\n",
    "    related_tickers = check_companies_in_text(current_news[\"title\"], stop_words)\n",
    "    for ticker in related_tickers.values():\n",
    "        if ticker not in interfacs_news_articles:\n",
    "            interfacs_news_articles[ticker] = []\n",
    "        interfacs_news_articles[ticker].append((current_news[\"title\"], current_news[\"datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ac5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Поможет распарсить 200 новостей на каждую акцию с trading view используя тикеры от тинькофф.\n",
    "#news = parse_url(\"https://news-headlines.tradingview.com/v2/view/headlines/symbol?client=web&lang=ru&streaming=true&symbol=MOEX%3ASBER\")\n",
    "\n",
    "#Вводим список компаний руками, нужно автоматическое преобразование к hex, потом просто перебираем страницы, до конца.\n",
    "#news = parse_url(\"https://www.rbc.ru/search/ajax/?tag=%D0%A1%D0%B1%D0%B5%D1%80%D0%B1%D0%B0%D0%BD%D0%BA&project=rbcnews&page=70\")\n",
    "\n",
    "#№https://alenka.capital/category/view_596/?date=01.02.2024\n",
    "\n",
    "for item in news:\n",
    "    try:\n",
    "        published_dt = datetime.strptime(item['published'], '%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"New article found: {item['header']} published at {item['published']}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтровать новости: аноснс событий, дайджест, лидеры роста и падаения"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
